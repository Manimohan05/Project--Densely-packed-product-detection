{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\Object Detection Course.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Did/Computer_Vision_Projects/Densely%20packed%20Product%20Detection/Object%20Detection%20Course.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Perform inference on the image\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Did/Computer_Vision_Projects/Densely%20packed%20Product%20Detection/Object%20Detection%20Course.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39mpath/to/your/yolov8n.pt\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Specify the correct path to the YOLOv5 model weights\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Did/Computer_Vision_Projects/Densely%20packed%20Product%20Detection/Object%20Detection%20Course.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m results \u001b[39m=\u001b[39m model(\u001b[39m\"\u001b[39;49m\u001b[39mObject tracking images and videos/veg.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Did/Computer_Vision_Projects/Densely%20packed%20Product%20Detection/Object%20Detection%20Course.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Save the annotated image with bounding boxes\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Did/Computer_Vision_Projects/Densely%20packed%20Product%20Detection/Object%20Detection%20Course.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m results\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39moutput.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:102\u001b[0m, in \u001b[0;36mYOLO.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, source\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(source, stream, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:198\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor:\n\u001b[0;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPredictorClass(overrides\u001b[39m=\u001b[39moverrides)\n\u001b[1;32m--> 198\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor\u001b[39m.\u001b[39;49msetup_model(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel)\n\u001b[0;32m    199\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# only update args if predictor is already setup\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m get_cfg(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs, overrides)\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\ultralytics\\yolo\\engine\\predictor.py:216\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup_model\u001b[39m(\u001b[39mself\u001b[39m, model):\n\u001b[1;32m--> 216\u001b[0m     device \u001b[39m=\u001b[39m select_device(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    217\u001b[0m     model \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmodel\n\u001b[0;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhalf \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m device\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# half precision only supported on CUDA\u001b[39;00m\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\ultralytics\\yolo\\utils\\torch_utils.py:95\u001b[0m, in \u001b[0;36mselect_device\u001b[1;34m(device, batch, newline, verbose)\u001b[0m\n\u001b[0;32m     93\u001b[0m space \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mlen\u001b[39m(s) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m i, d \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(devices):\n\u001b[1;32m---> 95\u001b[0m     p \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mget_device_properties(i)\n\u001b[0;32m     96\u001b[0m     s \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39mif\u001b[39;00m\u001b[39m \u001b[39mi\u001b[39m \u001b[39m\u001b[39m==\u001b[39m\u001b[39m \u001b[39m\u001b[39m0\u001b[39m\u001b[39m \u001b[39m\u001b[39melse\u001b[39;00m\u001b[39m \u001b[39mspace\u001b[39m}\u001b[39;00m\u001b[39mCUDA:\u001b[39m\u001b[39m{\u001b[39;00md\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m.\u001b[39mtotal_memory\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m(\u001b[39m1\u001b[39m\u001b[39m \u001b[39m\u001b[39m<<\u001b[39m\u001b[39m \u001b[39m\u001b[39m20\u001b[39m)\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39mMiB)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m  \u001b[39m# bytes to MB\u001b[39;00m\n\u001b[0;32m     97\u001b[0m arg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:449\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_properties\u001b[39m(device: _device_t) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    440\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \n\u001b[0;32m    442\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[39m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     _lazy_init()  \u001b[39m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    450\u001b[0m     device \u001b[39m=\u001b[39m _get_device_index(device, optional\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    451\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count():\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:304\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[0;32m    302\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m \u001b[39mfor\u001b[39;00m calls \u001b[39min\u001b[39;00m _lazy_seed_tracker\u001b[39m.\u001b[39;49mget_calls():\n\u001b[0;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m calls:\n\u001b[0;32m    306\u001b[0m         _queued_calls\u001b[39m.\u001b[39mappend(calls)\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:78\u001b[0m, in \u001b[0;36m_LazySeedTracker.get_calls\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[39m# update seed to be latest\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_order \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_seed_all_cb, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_seed_cb]\n\u001b[1;32m---> 78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_calls\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[0;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_order\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Perform inference on the image\n",
    "model = YOLO('path/to/your/yolov8n.pt')  # Specify the correct path to the YOLOv5 model weights\n",
    "results = model(\"Object tracking images and videos/veg.jpg\")\n",
    "\n",
    "# Save the annotated image with bounding boxes\n",
    "results.save(\"output.jpg\")\n",
    "\n",
    "# You can optionally display the saved image using OpenCV\n",
    "import cv2\n",
    "image = cv2.imread(\"output.jpg\")\n",
    "cv2.imshow(\"Annotated Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO with webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING  Ultralytics settings reset to defaults. This is normal and may be due to a recent ultralytics package update, but may have overwritten previous settings. \n",
      "View and update settings with 'yolo settings' or at 'C:\\Users\\RAJARETNAM SAIRISAN\\AppData\\Roaming\\Ultralytics\\settings.yaml'\n",
      "Ultralytics YOLOv8.0.43  Python-3.11.4 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 2060, 6144MiB)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import cvzone\n",
    "\n",
    "\n",
    "#cap = cv2.VideoCapture(0)   #turn camera 0- laptop own cam\n",
    "cap = cv2.VideoCapture(\"Object tracking images and videos/market.mp4\")\n",
    "cap.set(3,640)\n",
    "cap.set(4,480)\n",
    "\n",
    "model = YOLO('YOLOweights\\yolov8l.pt')\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()   # read img frame one by one\n",
    "    results = model(img, stream=True)\n",
    "    \n",
    "    for r in results:\n",
    "        boxes =r.boxes\n",
    "        for box in boxes:\n",
    "            x1,y1,x2,y2= box.xyxy[0]\n",
    "            x1,y1,x2,y2=int(x1),int(y1),int(x2),int(y2)\n",
    "            #print(x1,y1,x2,y2)\n",
    "\n",
    "            w,h=x2-x1,y2-y1\n",
    "            #cv2.rectangle(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "            cvzone.cornerRect(img, (x1, y1, w, h),colorR=(0,0,255),colorC=(0,255,0),t=5,rt=1)\n",
    "            \n",
    "            \n",
    "    \n",
    "    cv2.imshow(\"Image\",img)\n",
    "\n",
    "    key=cv2.waitKey(1)\n",
    "    # Check if the key pressed is '0'\n",
    "    if key == ord('0'):         \n",
    "        break  # Exit the loop if '0' is pressed\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Perform inference on the image\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.43  Python-3.11.4 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 2060, 6144MiB)\n",
      "YOLOv8l summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n",
      "\n",
      "0: 640x384 1 refrigerator, 98.7ms\n",
      "Speed: 24.0ms preprocess, 98.7ms inference, 554.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 35.9ms\n",
      "Speed: 0.0ms preprocess, 35.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 35.9ms\n",
      "Speed: 1.0ms preprocess, 35.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 34.9ms\n",
      "Speed: 1.0ms preprocess, 34.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 34.9ms\n",
      "Speed: 1.0ms preprocess, 34.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 33.9ms\n",
      "Speed: 1.0ms preprocess, 33.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 34.9ms\n",
      "Speed: 1.0ms preprocess, 34.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 25.9ms\n",
      "Speed: 1.0ms preprocess, 25.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 21.9ms\n",
      "Speed: 1.0ms preprocess, 21.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.0ms\n",
      "Speed: 0.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.0ms\n",
      "Speed: 0.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 21.9ms\n",
      "Speed: 1.0ms preprocess, 21.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 21.9ms\n",
      "Speed: 0.0ms preprocess, 21.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 24.0ms\n",
      "Speed: 1.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 1.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.0ms\n",
      "Speed: 1.0ms preprocess, 23.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.0ms\n",
      "Speed: 0.0ms preprocess, 23.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 1.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 24.0ms\n",
      "Speed: 1.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 21.9ms\n",
      "Speed: 0.0ms preprocess, 21.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 27.9ms\n",
      "Speed: 1.0ms preprocess, 27.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 21.9ms\n",
      "Speed: 1.0ms preprocess, 21.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 21.9ms\n",
      "Speed: 1.0ms preprocess, 21.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.0ms\n",
      "Speed: 1.0ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.0ms\n",
      "Speed: 0.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.0ms\n",
      "Speed: 1.0ms preprocess, 23.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.0ms\n",
      "Speed: 1.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.0ms\n",
      "Speed: 1.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 24.0ms\n",
      "Speed: 0.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 24.0ms\n",
      "Speed: 0.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 24.0ms\n",
      "Speed: 1.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 1.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 25.0ms\n",
      "Speed: 0.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 1.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 24.9ms\n",
      "Speed: 0.0ms preprocess, 24.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 24.9ms\n",
      "Speed: 0.0ms preprocess, 24.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 1 refrigerator, 21.9ms\n",
      "Speed: 1.0ms preprocess, 21.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 3 bottles, 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 15.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 3 bottles, 1 refrigerator, 22.0ms\n",
      "Speed: 0.0ms preprocess, 22.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 24.9ms\n",
      "Speed: 0.0ms preprocess, 24.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 1.0ms preprocess, 23.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.0ms\n",
      "Speed: 0.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 1.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 24.0ms\n",
      "Speed: 0.0ms preprocess, 24.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 0.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 24.9ms\n",
      "Speed: 1.0ms preprocess, 24.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.9ms\n",
      "Speed: 1.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 23.0ms\n",
      "Speed: 1.0ms preprocess, 23.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 refrigerator, 22.9ms\n",
      "Speed: 0.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import cvzone\n",
    "import math\n",
    "\n",
    "#cap = cv2.VideoCapture(0)   #turn camera 0- laptop own cam\n",
    "cap = cv2.VideoCapture(\"Object tracking images and videos/market.mp4\")\n",
    "cap.set(3,640)\n",
    "cap.set(4,480)\n",
    "\n",
    "model = YOLO('YOLOweights\\yolov8l.pt')\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()   # read img frame one by one\n",
    "    results = model(img, stream=True)\n",
    "    \n",
    "    for r in results:\n",
    "        boxes =r.boxes\n",
    "        for box in boxes:\n",
    "            x1,y1,x2,y2= box.xyxy[0]\n",
    "            x1,y1,x2,y2=int(x1),int(y1),int(x2),int(y2)\n",
    "            #print(x1,y1,x2,y2)\n",
    "\n",
    "            w,h=x2-x1,y2-y1\n",
    "            #cv2.rectangle(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "            cvzone.cornerRect(img, (x1, y1, w, h),colorR=(0,0,255),colorC=(0,255,0),t=5,rt=1)\n",
    "            conf=math.ceil(box.conf[0]*100)/100\n",
    "            #print(conf)\n",
    "            cvzone.putTextRect(img, f'{conf}' , (x1, y1+4),scale=1,thickness=2)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"Image\",img)\n",
    "\n",
    "    key=cv2.waitKey(1)\n",
    "    # Check if the key pressed is '0'\n",
    "    if key == ord('0'):         \n",
    "        break  # Exit the loop if '0' is pressed\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Perform inference on the image\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.43  Python-3.11.4 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 2060, 6144MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "0: 640x384 1 bottle, 21.9ms\n",
      "Speed: 1.0ms preprocess, 21.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 20.9ms\n",
      "Speed: 3.0ms preprocess, 20.9ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 5 bottles, 21.0ms\n",
      "Speed: 2.0ms preprocess, 21.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 4 bottles, 20.9ms\n",
      "Speed: 2.0ms preprocess, 20.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 7 bottles, 20.9ms\n",
      "Speed: 2.0ms preprocess, 20.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 6 bottles, 19.9ms\n",
      "Speed: 3.0ms preprocess, 19.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 4 bottles, 19.9ms\n",
      "Speed: 2.0ms preprocess, 19.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 3 bottles, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 17.9ms\n",
      "Speed: 2.0ms preprocess, 17.9ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 (no detections), 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 1 clock, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 1 clock, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 clock, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 2 clocks, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 1 clock, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 9.9ms\n",
      "Speed: 1.0ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 1 refrigerator, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 1 refrigerator, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 4 bottles, 1 refrigerator, 9.9ms\n",
      "Speed: 1.0ms preprocess, 9.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 5 bottles, 1 refrigerator, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 6 bottles, 1 refrigerator, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 4 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 5 bottles, 1 refrigerator, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 3 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 refrigerator, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 4 bottles, 1 refrigerator, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 3 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 4 bottles, 1 refrigerator, 7.9ms\n",
      "Speed: 2.0ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 7 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 5 bottles, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 4 bottles, 1 clock, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 2 clocks, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 1 bottle, 1 clock, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 clocks, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 clocks, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 3 bottles, 1 clock, 19.0ms\n",
      "Speed: 1.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 7 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 9 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 3 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 bottles, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 7 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 4 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 7 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 5 bottles, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 4 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 7 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 8 bottles, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 6 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 9 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 6 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 8 bottles, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 7 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 8 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 7 bottles, 1 refrigerator, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 9 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 9 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 7 bottles, 1 refrigerator, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 6 bottles, 1 refrigerator, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 3 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 3 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 5 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 4 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 4 bottles, 1 refrigerator, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 5 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 9 bottles, 1 refrigerator, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 6 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 7 bottles, 1 refrigerator, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 7 bottles, 1 refrigerator, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 8 bottles, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 9 bottles, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 12 bottles, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 12 bottles, 8.9ms\n",
      "Speed: 1.0ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 9 bottles, 1 refrigerator, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import cvzone\n",
    "import math\n",
    "\n",
    "#cap = cv2.VideoCapture(0)   #turn camera 0- laptop own cam\n",
    "cap = cv2.VideoCapture(\"Object tracking images and videos/market.mp4\")\n",
    "wid=640\n",
    "hei=480\n",
    "cap.set(3,wid)\n",
    "cap.set(4,hei)\n",
    "\n",
    "model = YOLO('models/yolov5.pt')\n",
    "\n",
    "# classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "#               \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "#               \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "#               \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "#               \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "#               \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "#               \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "#               \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "#               \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "#               \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "#               ]\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()   # read img frame one by one\n",
    "    results = model(img, stream=True)\n",
    "    \n",
    "    for r in results:\n",
    "        boxes =r.boxes\n",
    "        for box in boxes:\n",
    "            x1,y1,x2,y2= box.xyxy[0]\n",
    "            x1,y1,x2,y2=int(x1),int(y1),int(x2),int(y2)\n",
    "            #print(x1,y1,x2,y2)\n",
    "\n",
    "            w,h=x2-x1,y2-y1\n",
    "            #cv2.rectangle(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "            cvzone.cornerRect(img, (x1, y1, w, h),colorR=(0,0,255),colorC=(0,255,0),t=5,rt=1)\n",
    "            conf=math.ceil(box.conf[0]*100)/100\n",
    "            #print(conf)\n",
    "\n",
    "            cvzone.putTextRect(img, f'{conf}' , (x1, y1+4),scale=1,thickness=2)\n",
    "\n",
    "\n",
    "\n",
    "            cls=int(box.cls[0])\n",
    "            #print(cls)\n",
    "            #clsna=[\"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\"traffic light\", \"fire hydrant\", \"stop sign\"]\n",
    "            \n",
    "            #if (classNames[cls] in clsna ):\n",
    "\n",
    "            # cvzone.putTextRect(img, f'{classNames[cls]}' , (x2-20, y1+4),scale=1,thickness=2)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"Image\",img)\n",
    "\n",
    "    key=cv2.waitKey(1)\n",
    "    # Check if the key pressed is '0'\n",
    "    if key == ord('0'):         \n",
    "        break  # Exit the loop if '0' is pressed\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Perform inference on the image\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv8 requirement \"utils.utils\" not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  Command 'pip install \"utils.utils\"  ' returned non-zero exit status 1.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:340\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[1;34m(weight)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(file, map_location\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m), file  \u001b[39m# load\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\torch\\serialization.py:1028\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1028\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\torch\\serialization.py:1256\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1256\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1258\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\torch\\serialization.py:1061\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m-> 1061\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfind_class(mod_name, name)\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\models\\yolo.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39margparse\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDetect\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\models\\experimental.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# This file contains experimental modules\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCrossConv\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# Cross Convolution Downsample\u001b[39;00m\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\models\\common.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# This file contains modules common to various models\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mautopad\u001b[39m(k, p\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# kernel, padding\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[39m# Pad to 'same'\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils.utils'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\Object Detection Course.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Did/Computer_Vision_Projects/Densely%20packed%20Product%20Detection/Object%20Detection%20Course.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model_weights \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mYOLOweights/last_yolov5s_results.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Did/Computer_Vision_Projects/Densely%20packed%20Product%20Detection/Object%20Detection%20Course.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model_config \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msetup.cfg\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# or .cfg file if that's what you have\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Did/Computer_Vision_Projects/Densely%20packed%20Product%20Detection/Object%20Detection%20Course.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model \u001b[39m=\u001b[39m YOLO(model_weights,model_config)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Did/Computer_Vision_Projects/Densely%20packed%20Product%20Detection/Object%20Detection%20Course.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Did/Computer_Vision_Projects/Densely%20packed%20Product%20Detection/Object%20Detection%20Course.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     success, img \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()   \u001b[39m# read img frame one by one\u001b[39;00m\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:99\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[1;34m(self, model, type)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(model)\n\u001b[0;32m     98\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load(model)\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:128\u001b[0m, in \u001b[0;36mYOLO._load\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m    126\u001b[0m suffix \u001b[39m=\u001b[39m Path(weights)\u001b[39m.\u001b[39msuffix\n\u001b[0;32m    127\u001b[0m \u001b[39mif\u001b[39;00m suffix \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 128\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt \u001b[39m=\u001b[39m attempt_load_one_weight(weights)\n\u001b[0;32m    129\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverrides \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:394\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[1;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mattempt_load_one_weight\u001b[39m(weight, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fuse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    393\u001b[0m     \u001b[39m# Loads a single model weights\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     ckpt, weight \u001b[39m=\u001b[39m torch_safe_load(weight)  \u001b[39m# load ckpt\u001b[39;00m\n\u001b[0;32m    395\u001b[0m     args \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mDEFAULT_CFG_DICT, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mckpt[\u001b[39m'\u001b[39m\u001b[39mtrain_args\u001b[39m\u001b[39m'\u001b[39m]}  \u001b[39m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[0;32m    396\u001b[0m     model \u001b[39m=\u001b[39m (ckpt\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mema\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m ckpt[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()  \u001b[39m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:349\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[1;34m(weight)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mname \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    348\u001b[0m     check_requirements(e\u001b[39m.\u001b[39mname)  \u001b[39m# install missing module\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(file, map_location\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m), file\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\torch\\serialization.py:1028\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1026\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1027\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1028\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\torch\\serialization.py:1256\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1254\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1255\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1256\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1258\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1260\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\venv\\Lib\\site-packages\\torch\\serialization.py:1061\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1059\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m   1060\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m-> 1061\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfind_class(mod_name, name)\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\models\\yolo.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39margparse\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDetect\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      7\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, nc\u001b[39m=\u001b[39m\u001b[39m80\u001b[39m, anchors\u001b[39m=\u001b[39m()):  \u001b[39m# detection layer\u001b[39;00m\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\models\\experimental.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# This file contains experimental modules\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCrossConv\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# Cross Convolution Downsample\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, c1, c2, k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, s\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, g\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, e\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, shortcut\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m      9\u001b[0m         \u001b[39m# ch_in, ch_out, kernel, stride, groups, expansion, shortcut\u001b[39;00m\n",
      "File \u001b[1;32me:\\Did\\Computer_Vision_Projects\\Densely packed Product Detection\\models\\common.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# This file contains modules common to various models\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mautopad\u001b[39m(k, p\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# kernel, padding\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[39m# Pad to 'same'\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m p \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils.utils'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "import cv2\n",
    "import cvzone\n",
    "import math\n",
    "\n",
    "#cap = cv2.VideoCapture(0)   #turn camera 0- laptop own cam\n",
    "cap = cv2.VideoCapture(\"Object tracking images and videos/market.mp4\")\n",
    "wid=640\n",
    "hei=480\n",
    "cap.set(3,wid)\n",
    "cap.set(4,hei)\n",
    "\n",
    "model_weights = 'YOLOweights/yolov8l.pt'\n",
    "model_config = 'setup.cfg'  # or .cfg file if that's what you have\n",
    "\n",
    "\n",
    "model = YOLO(model_weights,model_config)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()   # read img frame one by one\n",
    "    results = model(img, stream=True)\n",
    "    \n",
    "    for r in results:\n",
    "        boxes =r.boxes\n",
    "        for box in boxes:\n",
    "            x1,y1,x2,y2= box.xyxy[0]\n",
    "            x1,y1,x2,y2=int(x1),int(y1),int(x2),int(y2)\n",
    "            #print(x1,y1,x2,y2)\n",
    "\n",
    "            w,h=x2-x1,y2-y1\n",
    "            #cv2.rectangle(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "            cvzone.cornerRect(img, (x1, y1, w, h),colorR=(0,0,255),colorC=(0,255,0),t=5,rt=1)\n",
    "            conf=math.ceil(box.conf[0]*100)/100\n",
    "            #print(conf)\n",
    "\n",
    "            cvzone.putTextRect(img, f'{conf}' , (x1, y1+4),scale=1,thickness=2)\n",
    "\n",
    "\n",
    "\n",
    "            cls=int(box.cls[0])\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"Image\",img)\n",
    "\n",
    "    key=cv2.waitKey(1)\n",
    "    # Check if the key pressed is '0'\n",
    "    if key == ord('0'):         \n",
    "        break  # Exit the loop if '0' is pressed\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Perform inference on the image\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
